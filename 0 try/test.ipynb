{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe31457",
   "metadata": {},
   "source": [
    "# Mental Health in Tech Survey Analysis\n",
    "Analysis of factors influencing mental health treatment seeking in tech.\n",
    "\n",
    "## Setup\n",
    "Import required libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6053917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1259, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/364658969276453932', creation_time=1762713856646, experiment_id='364658969276453932', last_update_time=1762713856646, lifecycle_stage='active', name='mental-health-tech-prediction', tags={'mlflow.experimentKind': 'custom_model_development'}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# MLflow imports\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, \n",
    "    precision_recall_curve, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"../data/raw/survey.csv\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Adjust if using different URI\n",
    "mlflow.set_experiment(\"mental-health-tech-prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061ba28",
   "metadata": {},
   "source": [
    "## Section 2: Initial Feature Engineering\n",
    "1. Clean and prepare features\n",
    "2. Handle gender categories\n",
    "3. Split features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52f7d9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types:\n",
      "Numeric: 1\n",
      "Categorical: 21\n"
     ]
    }
   ],
   "source": [
    "# Define target and initial feature selection\n",
    "target_col = \"treatment\"\n",
    "features_to_drop = ['Timestamp', 'Country', 'state', 'comments', target_col]\n",
    "\n",
    "# Prepare target (binary: Yes/No -> 1/0)\n",
    "y = df[target_col].map({\"Yes\": 1, \"No\": 0}).astype(int)\n",
    "X = df.drop(columns=features_to_drop)\n",
    "\n",
    "# Helper function for gender cleaning\n",
    "def clean_gender(gen):\n",
    "    s = str(gen).strip().lower()\n",
    "    s = re.sub(r\"[\\W_]+\", \" \", s).strip()\n",
    "    \n",
    "    if s in {\"m\", \"male\", \"man\", \"make\", \"mal\", \"malr\", \"msle\", \"masc\", \"mail\", \"boy\"}:\n",
    "        return \"Male\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"femake\", \"femail\", \"femme\", \"girl\"}:\n",
    "        return \"Female\"\n",
    "    return \"Other\"\n",
    "\n",
    "# Apply gender cleaning\n",
    "X['Gender'] = X['Gender'].apply(clean_gender)\n",
    "\n",
    "# Separate features by type\n",
    "numeric_features = X.select_dtypes(include=['number', 'bool']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "other_categoricals = [c for c in categorical_features if c != 'Gender']\n",
    "\n",
    "print(\"Feature types:\")\n",
    "print(f\"Numeric: {len(numeric_features)}\")\n",
    "print(f\"Categorical: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21bae5b",
   "metadata": {},
   "source": [
    "## Section 3: Preprocessing Pipeline Setup\n",
    "Define preprocessing steps for different feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0cebafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline components\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine into preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "# Model definitions\n",
    "models = {\n",
    "    'LogisticRegression': (\n",
    "        LogisticRegression(solver='liblinear', max_iter=5000),\n",
    "        {\n",
    "            'C': np.logspace(-3, 2, 20),\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "    ),\n",
    "    'RandomForest': (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 5, 10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    ),\n",
    "    'XGBoost': (\n",
    "        XGBClassifier(random_state=42),\n",
    "        {\n",
    "            'n_estimators': [200, 300, 400],\n",
    "            'max_depth': [3, 4, 5, 6],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0]\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fbea1b",
   "metadata": {},
   "source": [
    "## Section 4: Model Training & Initial Evaluation\n",
    "Train models with cross-validation and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40264d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/16 01:10:20 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/16 01:10:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run cv_search_LogisticRegression at: http://localhost:5000/#/experiments/364658969276453932/runs/2bcc9aeee5ba432fb003274289784a4b\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/364658969276453932\n",
      "\n",
      "Training RandomForest...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/16 01:10:33 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/16 01:10:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run cv_search_RandomForest at: http://localhost:5000/#/experiments/364658969276453932/runs/ce827944c0aa48feb052ce9705c6cbe7\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/364658969276453932\n",
      "\n",
      "Training XGBoost...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/16 01:10:41 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/16 01:10:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run cv_search_XGBoost at: http://localhost:5000/#/experiments/364658969276453932/runs/8019da6dbe06453abc2e78eb276e8953\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/364658969276453932\n",
      "\n",
      "Model Comparison:\n",
      "                model   roc_auc        f1  precision    recall\n",
      "2             XGBoost  0.821815  0.752776   0.749365  0.758268\n",
      "1        RandomForest  0.815084  0.760413   0.757337  0.766129\n",
      "0  LogisticRegression  0.811529  0.755670   0.748625  0.764505\n"
     ]
    }
   ],
   "source": [
    "def train_evaluate_model(X, y, model_name, model_tuple, cv=5, n_iter=20):\n",
    "    \"\"\"Train a model. If param grid is empty, fit pipeline and log CV metrics;\n",
    "    otherwise run RandomizedSearchCV. Returns fitted estimator or search object.\"\"\"\n",
    "    model, param_grid = model_tuple\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # prefix params for pipeline if provided\n",
    "    param_grid_prefixed = {f'model__{k}': v for k, v in (param_grid or {}).items()}\n",
    "\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': make_scorer(precision_score, zero_division=0),\n",
    "        'recall': make_scorer(recall_score, zero_division=0),\n",
    "        'f1': make_scorer(f1_score, zero_division=0),\n",
    "        'roc_auc': 'roc_auc'\n",
    "    }\n",
    "\n",
    "    run_name = f\"cv_search_{model_name}\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_param('dataset_shape', X.shape)\n",
    "        mlflow.log_param('cv_folds', cv)\n",
    "\n",
    "        # If no hyperparameter grid provided, do a single fit + CV evaluation\n",
    "        if not param_grid_prefixed:\n",
    "            # Fit pipeline on full data\n",
    "            pipeline.fit(X, y)\n",
    "            # Evaluate with cross-val (cv) for metrics to log\n",
    "            cv_obj = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "            metrics = {}\n",
    "            for metric_name, scorer in [('roc_auc','roc_auc'), ('accuracy','accuracy')]:\n",
    "                try:\n",
    "                    scores = []\n",
    "                    if metric_name == 'roc_auc':\n",
    "                        # use cross_val_score with scoring string\n",
    "                        scores = pd.Series(np.mean(pd.np.zeros(1))) if False else None\n",
    "                    # fallback to cross_val_score for available scoring strings\n",
    "                    from sklearn.model_selection import cross_val_score\n",
    "                    sc = cross_val_score(pipeline, X, y, cv=cv_obj, scoring=scorer, n_jobs=-1)\n",
    "                    metrics[f'cv_mean_{metric_name}'] = float(np.mean(sc))\n",
    "                except Exception:\n",
    "                    # skip metric if CV scoring failed\n",
    "                    metrics[f'cv_mean_{metric_name}'] = np.nan\n",
    "\n",
    "            # Log metrics and model\n",
    "            mlflow.log_metrics(metrics)\n",
    "            mlflow.sklearn.log_model(pipeline, f'model_{model_name}')\n",
    "            return pipeline\n",
    "\n",
    "        # Otherwise perform randomized search\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline, param_grid_prefixed, n_iter=n_iter, cv=cv,\n",
    "            scoring=scoring, refit='roc_auc',\n",
    "            n_jobs=-1, random_state=42, verbose=1\n",
    "        )\n",
    "        search.fit(X, y)\n",
    "\n",
    "        # Log best parameters and metrics (guarded)\n",
    "        try:\n",
    "            mlflow.log_params({f'best_{k}': v for k, v in search.best_params_.items()})\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            bi = search.best_index_\n",
    "            mlflow.log_metric('best_roc_auc', float(search.cv_results_['mean_test_roc_auc'][bi]))\n",
    "            mlflow.log_metric('best_f1', float(search.cv_results_['mean_test_f1'][bi]))\n",
    "            mlflow.log_metric('best_precision', float(search.cv_results_['mean_test_precision'][bi]))\n",
    "            mlflow.log_metric('best_recall', float(search.cv_results_['mean_test_recall'][bi]))\n",
    "        except Exception:\n",
    "            # if cv_results_ keys are missing, skip logging\n",
    "            pass\n",
    "\n",
    "        # Log best model\n",
    "        try:\n",
    "            mlflow.sklearn.log_model(search.best_estimator_, f'model_{model_name}')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return search\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model_tuple in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    search = train_evaluate_model(X, y, name, model_tuple)\n",
    "    trained_models[name] = search\n",
    "    \n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'roc_auc': search.cv_results_['mean_test_roc_auc'][search.best_index_],\n",
    "        'f1': search.cv_results_['mean_test_f1'][search.best_index_],\n",
    "        'precision': search.cv_results_['mean_test_precision'][search.best_index_],\n",
    "        'recall': search.cv_results_['mean_test_recall'][search.best_index_]\n",
    "    })\n",
    "\n",
    "# Show results\n",
    "results_df = pd.DataFrame(results).sort_values('roc_auc', ascending=False)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355870fd",
   "metadata": {},
   "source": [
    "## Section 5: Feature Selection & Final Model\n",
    "Identify and remove noisy features, then retrain best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "109f5639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature selection process...\n",
      "Using XGBoost for feature importance analysis\n",
      "\n",
      "Removing 3 noisy features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/16 01:10:49 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/16 01:10:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run final_model at: http://localhost:5000/#/experiments/364658969276453932/runs/ccce6cbda4b140e682b3b25febd2e46a\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/364658969276453932\n",
      "\n",
      "Experiment tracking completed. Check MLflow UI for detailed results.\n"
     ]
    }
   ],
   "source": [
    "def identify_noisy_features(X, y, threshold=0.01):\n",
    "    \"\"\"Identify low-importance features using permutation importance\"\"\"\n",
    "    try:\n",
    "        # Get best model from initial evaluation\n",
    "        best_model_name = results_df.iloc[0]['model']\n",
    "        best_pipeline = trained_models[best_model_name].best_estimator_\n",
    "        print(f\"Using {best_model_name} for feature importance analysis\")\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # Get preprocessor and model\n",
    "        preprocess = best_pipeline.named_steps['preprocess']\n",
    "        model = best_pipeline.named_steps['model']\n",
    "\n",
    "        # Fit preprocessor and transform\n",
    "        preprocess.fit(X_train)\n",
    "        X_train_processed = preprocess.transform(X_train)\n",
    "        X_val_processed = preprocess.transform(X_val)\n",
    "\n",
    "        # Fit model on processed data\n",
    "        model.fit(X_train_processed, y_train)\n",
    "\n",
    "        # Compute permutation importance on processed validation data\n",
    "        result = permutation_importance(\n",
    "            model, X_val_processed, y_val, n_repeats=10, random_state=42, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Attempt to get feature names from the preprocessor\n",
    "        try:\n",
    "            feature_names = preprocess.get_feature_names_out()\n",
    "        except Exception:\n",
    "            feature_names = [f'feature_{i}' for i in range(X_val_processed.shape[1])]\n",
    "\n",
    "        # Map importance back to original columns by matching substrings\n",
    "        original_importance = {}\n",
    "        for col in X.columns:\n",
    "            col_imp = []\n",
    "            for idx, feat in enumerate(feature_names):\n",
    "                if col.lower() in feat.lower() or col.lower().replace(' ', '_') in feat.lower():\n",
    "                    col_imp.append(result.importances_mean[idx])\n",
    "            original_importance[col] = float(max(col_imp)) if col_imp else 0.0\n",
    "\n",
    "        importances = pd.Series(original_importance)\n",
    "\n",
    "        # Save plot artifact\n",
    "        import os\n",
    "        os.makedirs('mlruns', exist_ok=True)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        importances.sort_values().plot(kind='barh')\n",
    "        plt.title('Feature Importance by Original Column')\n",
    "        plt.xlabel('Mean Importance (Permutation)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('mlruns/feature_importance.png')\n",
    "        plt.close()\n",
    "\n",
    "        noisy_cols = importances[importances < threshold].index.tolist()\n",
    "        return noisy_cols, importances\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature importance calculation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [], None\n",
    "\n",
    "# Run final model training with MLflow tracking (use best params, do NOT re-run RandomizedSearchCV)\n",
    "with mlflow.start_run(run_name='final_model') as run:\n",
    "    print('Starting feature selection process...')\n",
    "    noisy_features, importances = identify_noisy_features(X, y, threshold=0.001)\n",
    "\n",
    "    if noisy_features:\n",
    "        print(f\"\\nRemoving {len(noisy_features)} noisy features...\")\n",
    "        X_clean = X.drop(columns=noisy_features)\n",
    "        mlflow.log_param('removed_features', str(noisy_features))\n",
    "    else:\n",
    "        print('\\nNo features to remove.')\n",
    "        X_clean = X.copy()\n",
    "        mlflow.log_param('removed_features', 'none')\n",
    "\n",
    "    mlflow.log_param('final_feature_count', X_clean.shape[1])\n",
    "\n",
    "    if importances is not None:\n",
    "        mlflow.log_artifact('mlruns/feature_importance.png')\n",
    "\n",
    "    # Rebuild preprocessor for cleaned data\n",
    "    numeric_features = X_clean.select_dtypes(include=['number', 'bool']).columns.tolist()\n",
    "    categorical_features = X_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_pipeline, numeric_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ], remainder='drop')\n",
    "\n",
    "    # Build final estimator using best hyperparameters from earlier search (do not run CV again)\n",
    "    best_model_name = results_df.iloc[0]['model']\n",
    "    best_search = trained_models[best_model_name]\n",
    "\n",
    "    # Extract model-level params (keys like 'model__param') and construct a fresh estimator\n",
    "    best_params = {}\n",
    "    if hasattr(best_search, 'best_params_') and best_search.best_params_:\n",
    "        # remove 'model__' prefix\n",
    "        best_params = {k.replace('model__', ''): v for k, v in best_search.best_params_.items()}\n",
    "\n",
    "    base_estimator = models[best_model_name][0]  # an instance from models dict\n",
    "    EstimatorClass = base_estimator.__class__\n",
    "    try:\n",
    "        final_estimator = EstimatorClass(**best_params) if best_params else EstimatorClass()\n",
    "    except Exception as e:\n",
    "        print('Warning: could not instantiate estimator with best_params, falling back to default. Error:', e)\n",
    "        final_estimator = EstimatorClass()\n",
    "\n",
    "    final_pipeline = Pipeline([('preprocess', preprocessor), ('model', final_estimator)])\n",
    "    # Fit final pipeline on cleaned full data\n",
    "    final_pipeline.fit(X_clean, y)\n",
    "    # Log final pipeline as model artifact\n",
    "    try:\n",
    "        mlflow.sklearn.log_model(final_pipeline, f'final_model_{best_model_name}')\n",
    "    except Exception as e:\n",
    "        print('Could not log final model to MLflow:', e)\n",
    "\n",
    "    # Final evaluation on holdout split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_clean, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    y_pred = final_pipeline.predict(X_te)\n",
    "    # try predict_proba, fallback to decision_function scaled to [0,1]\n",
    "    if hasattr(final_pipeline, 'predict_proba'):\n",
    "        y_prob = final_pipeline.predict_proba(X_te)[:, 1]\n",
    "    else:\n",
    "        try:\n",
    "            scores = final_pipeline.decision_function(X_te)\n",
    "            y_prob = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
    "        except Exception:\n",
    "            y_prob = np.zeros(len(y_te))\n",
    "\n",
    "    # Log final metrics\n",
    "    mlflow.log_metrics({\n",
    "        'final_accuracy': float(accuracy_score(y_te, y_pred)),\n",
    "        'final_precision': float(precision_score(y_te, y_pred, zero_division=0)),\n",
    "        'final_recall': float(recall_score(y_te, y_pred, zero_division=0)),\n",
    "        'final_f1': float(f1_score(y_te, y_pred, zero_division=0)),\n",
    "        'final_roc_auc': float(roc_auc_score(y_te, y_prob)) if y_prob.sum() else float('nan')\n",
    "    })\n",
    "\n",
    "    # Save and log ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_te, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc_score(y_te, y_prob):.3f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve - Final Model')\n",
    "        plt.legend()\n",
    "        plt.savefig('roc_curve.png')\n",
    "        plt.close()\n",
    "        mlflow.log_artifact('roc_curve.png')\n",
    "    except Exception as e:\n",
    "        print('Could not create/log ROC curve:', e)\n",
    "\n",
    "    # Persist feature selection results\n",
    "    try:\n",
    "        with open('feature_selection_results.txt', 'w', encoding='utf-8') as fh:\n",
    "            fh.write('Removed features:\\n')\n",
    "            for f in noisy_features:\n",
    "                fh.write(f'- {f}\\n')\n",
    "            fh.write('\\nRetained features:\\n')\n",
    "            for f in X_clean.columns:\n",
    "                fh.write(f'- {f}\\n')\n",
    "        mlflow.log_artifact('feature_selection_results.txt')\n",
    "    except Exception as e:\n",
    "        print('Could not write/log feature_selection_results.txt:', e)\n",
    "\n",
    "print('\\nExperiment tracking completed. Check MLflow UI for detailed results.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b40cfb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Model Hyperparameters Report ===\n",
      "Best model (per earlier search): XGBoost\n",
      "\n",
      "> Best params from earlier search (model-level):\n",
      "{'colsample_bytree': 1.0,\n",
      " 'learning_rate': 0.01,\n",
      " 'max_depth': 6,\n",
      " 'n_estimators': 400,\n",
      " 'subsample': 0.8}\n",
      "\n",
      "Final estimator class: XGBClassifier\n",
      "\n",
      "> Hyperparameters actually set on the FINAL estimator (authoritative):\n",
      "{'base_score': None,\n",
      " 'booster': None,\n",
      " 'callbacks': None,\n",
      " 'colsample_bylevel': None,\n",
      " 'colsample_bynode': None,\n",
      " 'colsample_bytree': 1.0,\n",
      " 'device': None,\n",
      " 'early_stopping_rounds': None,\n",
      " 'enable_categorical': False,\n",
      " 'eval_metric': None,\n",
      " 'feature_types': None,\n",
      " 'feature_weights': None,\n",
      " 'gamma': None,\n",
      " 'grow_policy': None,\n",
      " 'importance_type': None,\n",
      " 'interaction_constraints': None,\n",
      " 'learning_rate': 0.01,\n",
      " 'max_bin': None,\n",
      " 'max_cat_threshold': None,\n",
      " 'max_cat_to_onehot': None,\n",
      " 'max_delta_step': None,\n",
      " 'max_depth': 6,\n",
      " 'max_leaves': None,\n",
      " 'min_child_weight': None,\n",
      " 'missing': nan,\n",
      " 'monotone_constraints': None,\n",
      " 'multi_strategy': None,\n",
      " 'n_estimators': 400,\n",
      " 'n_jobs': None,\n",
      " 'num_parallel_tree': None,\n",
      " 'objective': 'binary:logistic',\n",
      " 'random_state': None,\n",
      " 'reg_alpha': None,\n",
      " 'reg_lambda': None,\n",
      " 'sampling_method': None,\n",
      " 'scale_pos_weight': None,\n",
      " 'subsample': 0.8,\n",
      " 'tree_method': None,\n",
      " 'validate_parameters': None,\n",
      " 'verbosity': None}\n",
      "\n",
      "> Differences between CV best params and final estimator params (CV -> FINAL):\n",
      "{'base_score': ('<missing_in_CV>', None),\n",
      " 'booster': ('<missing_in_CV>', None),\n",
      " 'callbacks': ('<missing_in_CV>', None),\n",
      " 'colsample_bylevel': ('<missing_in_CV>', None),\n",
      " 'colsample_bynode': ('<missing_in_CV>', None),\n",
      " 'device': ('<missing_in_CV>', None),\n",
      " 'early_stopping_rounds': ('<missing_in_CV>', None),\n",
      " 'enable_categorical': ('<missing_in_CV>', False),\n",
      " 'eval_metric': ('<missing_in_CV>', None),\n",
      " 'feature_types': ('<missing_in_CV>', None),\n",
      " 'feature_weights': ('<missing_in_CV>', None),\n",
      " 'gamma': ('<missing_in_CV>', None),\n",
      " 'grow_policy': ('<missing_in_CV>', None),\n",
      " 'importance_type': ('<missing_in_CV>', None),\n",
      " 'interaction_constraints': ('<missing_in_CV>', None),\n",
      " 'max_bin': ('<missing_in_CV>', None),\n",
      " 'max_cat_threshold': ('<missing_in_CV>', None),\n",
      " 'max_cat_to_onehot': ('<missing_in_CV>', None),\n",
      " 'max_delta_step': ('<missing_in_CV>', None),\n",
      " 'max_leaves': ('<missing_in_CV>', None),\n",
      " 'min_child_weight': ('<missing_in_CV>', None),\n",
      " 'missing': ('<missing_in_CV>', nan),\n",
      " 'monotone_constraints': ('<missing_in_CV>', None),\n",
      " 'multi_strategy': ('<missing_in_CV>', None),\n",
      " 'n_jobs': ('<missing_in_CV>', None),\n",
      " 'num_parallel_tree': ('<missing_in_CV>', None),\n",
      " 'objective': ('<missing_in_CV>', 'binary:logistic'),\n",
      " 'random_state': ('<missing_in_CV>', None),\n",
      " 'reg_alpha': ('<missing_in_CV>', None),\n",
      " 'reg_lambda': ('<missing_in_CV>', None),\n",
      " 'sampling_method': ('<missing_in_CV>', None),\n",
      " 'scale_pos_weight': ('<missing_in_CV>', None),\n",
      " 'tree_method': ('<missing_in_CV>', None),\n",
      " 'validate_parameters': ('<missing_in_CV>', None),\n",
      " 'verbosity': ('<missing_in_CV>', None)}\n",
      "\n",
      "=== End of Report ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Inspect final model hyperparameters (XGBoost / scikit-learn) ---\n",
    "from pprint import pprint\n",
    "\n",
    "def _strip_model_prefix(params):\n",
    "    \"\"\"Remove 'model__' prefixes coming from Grid/RandomizedSearch inside a Pipeline.\"\"\"\n",
    "    return {k.replace('model__', ''): v for k, v in params.items()}\n",
    "\n",
    "print(\"\\n=== Final Model Hyperparameters Report ===\")\n",
    "\n",
    "# 1) Identify the best model name used throughout the script\n",
    "best_model_name = None\n",
    "try:\n",
    "    best_model_name = results_df.iloc[0]['model']\n",
    "    print(f\"Best model (per earlier search): {best_model_name}\")\n",
    "except Exception as e:\n",
    "    print(\"Could not determine best model name from results_df:\", e)\n",
    "\n",
    "# 2) Extract best search object and its tuned params (from the earlier CV)\n",
    "best_params_from_search = {}\n",
    "try:\n",
    "    if best_model_name is not None and 'trained_models' in globals():\n",
    "        best_search = trained_models[best_model_name]\n",
    "        if hasattr(best_search, 'best_params_') and best_search.best_params_:\n",
    "            best_params_from_search = _strip_model_prefix(best_search.best_params_)\n",
    "            print(\"\\n> Best params from earlier search (model-level):\")\n",
    "            pprint(best_params_from_search)\n",
    "        else:\n",
    "            print(\"\\n> No best_params_ found on the stored search object.\")\n",
    "    else:\n",
    "        print(\"\\n> Could not access trained_models[best_model_name].\")\n",
    "except Exception as e:\n",
    "    print(\"\\nError while reading best_params_ from search object:\", e)\n",
    "\n",
    "# 3) Read the actual estimator that ended up inside the final pipeline\n",
    "final_estimator = None\n",
    "try:\n",
    "    if 'final_pipeline' in globals():\n",
    "        final_estimator = final_pipeline.named_steps['model']\n",
    "        print(f\"\\nFinal estimator class: {type(final_estimator).__name__}\")\n",
    "    else:\n",
    "        print(\"\\nNo 'final_pipeline' found in memory.\")\n",
    "except Exception as e:\n",
    "    print(\"\\nError accessing final_pipeline.named_steps['model']:\", e)\n",
    "\n",
    "# 4) Print the final estimator's effective hyperparameters (authoritative)\n",
    "try:\n",
    "    if final_estimator is not None and hasattr(final_estimator, \"get_params\"):\n",
    "        final_params = final_estimator.get_params(deep=False)\n",
    "        print(\"\\n> Hyperparameters actually set on the FINAL estimator (authoritative):\")\n",
    "        pprint(final_params)\n",
    "    else:\n",
    "        print(\"\\nFinal estimator not available to read params from.\")\n",
    "except Exception as e:\n",
    "    print(\"\\nError reading params from final estimator:\", e)\n",
    "\n",
    "# 5) Optional: show differences between CV best params and final estimator params\n",
    "try:\n",
    "    if final_estimator is not None and best_params_from_search:\n",
    "        final_params = final_estimator.get_params(deep=False)\n",
    "        diffs = {k: (best_params_from_search.get(k, \"<missing_in_CV>\"), final_params.get(k, \"<missing_in_final>\"))\n",
    "                 for k in set(best_params_from_search) | set(final_params)}\n",
    "        changed = {k: v for k, v in diffs.items() if v[0] != v[1]}\n",
    "        if changed:\n",
    "            print(\"\\n> Differences between CV best params and final estimator params (CV -> FINAL):\")\n",
    "            pprint(changed)\n",
    "        else:\n",
    "            print(\"\\n> CV best params match the final estimator params.\")\n",
    "except Exception as e:\n",
    "    print(\"\\nError computing CV vs FINAL parameter differences:\", e)\n",
    "\n",
    "print(\"\\n=== End of Report ===\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
